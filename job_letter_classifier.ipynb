{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n9rXQhPfLvIQ",
        "VpfGDnk7bEGR"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Job Classifier System\n"
      ],
      "metadata": {
        "id": "kaoBv5D9Lgo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: running the entire code takes about 60 minutes.\n",
        "*   Import CSV: ~20 minutes\n",
        "*   Train Job Type algorithm: ~20 minutes.\n",
        "*   Train Job Category algorithm: ~20 minutes.\n"
      ],
      "metadata": {
        "id": "0vxYeeZZkTD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries\n"
      ],
      "metadata": {
        "id": "n9rXQhPfLvIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33b9kvRs4IHM"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install -q cohere\n",
        "!pip install -q tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General purpose\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import openai\n",
        "import io\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data preparation\n",
        "from dateutil import parser\n",
        "import string\n",
        "\n",
        "# Modelling\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding"
      ],
      "metadata": {
        "id": "5MFKoJ4Z51Ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification Algorithms"
      ],
      "metadata": {
        "id": "zaQugMyXMGD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n"
      ],
      "metadata": {
        "id": "VpfGDnk7bEGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final csv file is saved as `updated3_postings.csv`. The code in this section is for demonstration purposes only.\n",
        "To upload the final csv and ran the next sections of the notebook, use the box below with first line `from google.colab import files` (loading time: ~20 minutes)."
      ],
      "metadata": {
        "id": "Idz_nh5P_idE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "dMQ8cKR9XZrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Code for demomstration purposes only. ###\n",
        "\n",
        "# processing the CSV files into a useful version\n",
        "jobs_file=pd.read_csv(\"job_postings.csv\")\n",
        "jobs2 = pd.read_csv('updated_postings.csv')\n",
        "new_column_names = {'job_id': 'job_no','title': 'job_description','formatted_work_type': 'job_type',}\n",
        "jobs2.rename(columns=new_column_names, inplace=True)\n",
        "jobs2.to_csv('updated2_postings.csv', index=False)\n",
        "jobs3 = pd.read_csv('updated2_postings.csv')\n",
        "\n",
        "# keywords per each category\n",
        "it_keywords = ['software', 'developer', 'engineer', 'programmer', 'IT', 'data', 'analyst', 'database', 'network', 'web', 'cloud', 'java', 'python', 'javascript', 'frontend', 'backend', 'full stack', 'devops', 'systems', 'security']\n",
        "marketing_keywords = ['marketing', 'manager', 'advertising', 'sales', 'digital', 'brand', 'social media', 'communications', 'public relations', 'content', 'market research', 'branding', 'campaign', 'SEO', 'SEM', 'analytics', 'email marketing', 'event planning', 'copywriting', 'creative']\n",
        "finance_keywords = ['finance', 'accountant', 'analyst', 'auditor', 'investment', 'banking', 'financial', 'audit', 'tax', 'risk management', 'financial planning', 'credit analysis', 'treasury', 'actuary', 'forensic accounting', 'bookkeeping', 'payroll', 'financial modeling', 'wealth management']\n",
        "customer_service_keywords = ['customer service', 'support', 'representative', 'client relations', 'help desk', 'call center', 'customer success', 'customer satisfaction', 'customer experience', 'service desk', 'complaint resolution', 'ticketing system', 'client support', 'client interaction', 'customer communication', 'escalation management', 'customer feedback', 'client support', 'service delivery']\n",
        "healthcare_keywords = ['healthcare','care', 'nurse', 'doctor', 'medical', 'hospital', 'health', 'clinical', 'pharmacy', 'medical billing', 'health information management', 'patient care', 'nursing', 'surgery', 'medical research', 'healthcare administration', 'medical imaging', 'pharmaceutical', 'epidemiology', 'public health', 'radiology']\n",
        "education_keywords = ['education', 'teacher', 'professor', 'instructor', 'tutor', 'academic', 'learning', 'school', 'curriculum development', 'educational technology', 'teaching assistant', 'classroom management', 'e-learning', 'educational research', 'student affairs', 'special education', 'educational leadership', 'pedagogy', 'instructional design']\n",
        "engineering_keywords = ['engineering', 'engineer', 'mechanical', 'electrical', 'civil', 'structural', 'automation', 'chemical', 'aerospace', 'biomedical', 'industrial', 'environmental', 'process', 'systems', 'quality', 'materials', 'control systems', 'manufacturing', 'robotics', 'energy']\n",
        "administration_keywords = ['administration', 'manager', 'coordinator', 'office', 'administrative', 'office manager', 'operations', 'administrative assistant', 'executive assistant', 'office administrator', 'office coordinator', 'office operations', 'office support', 'office procedures', 'office efficiency', 'office management', 'front desk', 'receptionist', 'secretary']\n",
        "sales_keywords = ['sales', 'representative', 'account executive', 'business development', 'sales manager', 'sales associate', 'client acquisition', 'sales coordinator', 'sales support', 'sales operations', 'sales strategy', 'customer acquisition', 'sales funnel', 'cold calling', 'lead generation', 'sales negotiation', 'client relationship', 'sales targets', 'sales forecasting', 'sales presentation']\n",
        "research_keywords = ['research', 'scientist', 'analyst', 'researcher', 'data analysis', 'research analyst', 'experimental', 'research scientist', 'statistical analysis', 'market research', 'quantitative research', 'qualitative research', 'research design', 'data collection', 'data interpretation', 'survey design', 'research methodologies', 'experimental design', 'scientific research']\n",
        "creative_keywords = ['creative', 'designer', 'artist', 'writer', 'editor', 'graphic design', 'illustrator', 'content creation', 'creative director', 'art director', 'copywriter', 'visual arts', 'creative writing', 'storytelling', 'digital art', 'branding design', 'multimedia design', 'motion graphics', 'creative strategy']\n",
        "legal_keywords = ['legal', 'lawyer', 'attorney', 'paralegal', 'legal advisor', 'law clerk', 'legal consultant', 'law enforcement', 'legal secretary', 'corporate law', 'criminal law', 'civil law', 'family law', 'intellectual property law', 'legal research', 'legal writing', 'court proceedings', 'contract law', 'litigation', 'legal compliance']\n",
        "human_resources_keywords = ['human resources', 'HR', 'recruiter', 'personnel', 'talent acquisition', 'HR manager', 'employee relations', 'human resource management', 'staffing', 'HR coordinator', 'workforce planning', 'compensation', 'benefits administration', 'employee engagement', 'performance management', 'training and development', 'organizational development', 'HR policies', 'talent management']\n",
        "manufacturing_keywords = ['manufacturing', 'production', 'operator', 'technician', 'assembly', 'manufacturing engineer', 'quality control', 'production supervisor', 'process improvement', 'lean manufacturing', 'six sigma', 'production planning', 'machine operation', 'materials management', 'maintenance technician', 'operations management', 'supply chain', 'continuous improvement', 'automation', 'product development']\n",
        "consulting_keywords = ['consultant', 'advisor', 'consulting', 'expert', 'strategist', 'business consultant', 'management consulting', 'strategy consultant', 'financial consultant', 'technology consultant', 'HR consultant', 'marketing consultant', 'operations consultant', 'IT consultant', 'risk management consultant', 'organizational consultant', 'change management consultant', 'business analysis', 'business strategy', 'business process improvement']\n",
        "logistics_keywords = ['logistics', 'supply chain', 'warehouse', 'dispatcher', 'shipping', 'logistics coordinator', 'inventory management', 'distribution', 'supply chain management', 'logistics manager', 'shipping and receiving', 'inventory control', 'logistics planning', 'order fulfillment', 'transportation', 'logistics operations', 'procurement', 'supply chain optimization', 'freight']\n",
        "real_estate_keywords = ['real estate', 'property', 'realtor', 'broker', 'appraiser', 'real estate agent', 'property management', 'real estate development', 'commercial real estate', 'residential real estate', 'real estate finance', 'property valuation', 'real estate transactions', 'real estate law', 'real estate marketing', 'real estate investment', 'leasing agent', 'land development', 'real estate appraisal', 'property leasing']\n",
        "jobs3['category'] = 'Uncategorized'\n",
        "\n",
        "# checking keywords in job description to assign category\n",
        "for index, row in jobs3.iterrows():\n",
        "\n",
        "    title = row['job_description'].lower()\n",
        "\n",
        "    if any(keyword in title for keyword in it_keywords):\n",
        "        jobs3.at[index, 'category'] = 'IT'\n",
        "    elif any(keyword in title for keyword in marketing_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Marketing'\n",
        "    elif any(keyword in title for keyword in finance_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Finance'\n",
        "    elif any(keyword in title for keyword in customer_service_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Customer Service'\n",
        "    elif any(keyword in title for keyword in healthcare_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Healthcare'\n",
        "    elif any(keyword in title for keyword in education_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Education'\n",
        "    elif any(keyword in title for keyword in engineering_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Engineering'\n",
        "    elif any(keyword in title for keyword in administration_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Administration'\n",
        "    elif any(keyword in title for keyword in sales_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Sales'\n",
        "    elif any(keyword in title for keyword in research_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Research'\n",
        "    elif any(keyword in title for keyword in creative_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Creative'\n",
        "    elif any(keyword in title for keyword in legal_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Legal'\n",
        "    elif any(keyword in title for keyword in human_resources_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Human Resources'\n",
        "    elif any(keyword in title for keyword in manufacturing_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Manufacturing'\n",
        "    elif any(keyword in title for keyword in consulting_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Consulting'\n",
        "    elif any(keyword in title for keyword in logistics_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Logistics'\n",
        "    elif any(keyword in title for keyword in real_estate_keywords):\n",
        "        jobs3.at[index, 'category'] = 'Real Estate'\n",
        "\n",
        "jobs3.to_csv('updated3_postings.csv', index=False)\n",
        "\n",
        "def is_valid_date(date_str):\n",
        "    try:\n",
        "        parser.parse(date_str)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def date_removal(data):\n",
        "    new_list = [' '.join([w for w in line.split() if not is_valid_date(w)]) for line in data]\n",
        "    return (new_list[0])\n",
        "\n",
        "def stemmer_and_stopWord(doc):\n",
        "    doc= nlp(doc)\n",
        "    token_list = []\n",
        "    for token in doc:\n",
        "      lemma = token.lemma_\n",
        "      if lemma == '-PRON-' or lemma == 'be':\n",
        "        lemma = token.text\n",
        "      token_list.append(lemma)\n",
        "    stemmed = token_list\n",
        "    filtered_sentence =[]\n",
        "    for word in stemmed[:100]:\n",
        "        lexeme = nlp.vocab[word]\n",
        "        if lexeme.is_stop == False:\n",
        "            filtered_sentence.append(word)\n",
        "    return (' '.join(filtered_sentence))\n",
        "\n",
        "def normaliz(filtered_sentence):\n",
        "    words = [str(word).lower() for word in filtered_sentence.split()]\n",
        "    return  ' '.join(words[:100])\n",
        "\n",
        "def numbers_removal(data):\n",
        "    s = [data]\n",
        "    result = ''.join([i for i in s if not i.isdigit()])\n",
        "    return (result)\n",
        "\n",
        "def punch_removal(words):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    stripped = [w.translate(table) for w in [words]]\n",
        "    return re.sub(' +', ' ', stripped[:100][0])\n",
        "\n",
        "def cleaner(data):\n",
        "    string = [data]\n",
        "    string = date_removal(string)\n",
        "    string = punch_removal(string)\n",
        "    string = stemmer_and_stopWord(string)\n",
        "    string = normaliz(string)\n",
        "    return string"
      ],
      "metadata": {
        "id": "BLpmW0znMKCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling\n"
      ],
      "metadata": {
        "id": "2XPRy_MueQi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we prepare the input to our models by importing the job data, engineering features, and tokenizing the text."
      ],
      "metadata": {
        "id": "P0DNPyLvAfYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the CSV\n",
        "train_df = pd.read_csv(io.BytesIO(uploaded['updated3_postings.csv']))"
      ],
      "metadata": {
        "id": "yIKnKrSJdRaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature engineering\n",
        "train_df['word_count'] = train_df[\"job_description\"].apply(lambda x: len(str(x).split(\" \")))\n",
        "train_df['char_count'] = train_df[\"job_description\"].apply(lambda x:sum(len(word) for word in str(x).split(\" \")))\n",
        "train_df['sentence_count'] = train_df[\"job_description\"].apply(lambda x: len(str(x).split(\".\")))\n",
        "train_df['avg_word_length'] = train_df['char_count'] / train_df['word_count']\n",
        "train_df['avg_sentence_lenght'] = train_df['word_count'] / train_df['sentence_count']\n",
        "\n",
        "# Check for possible division by zero\n",
        "train_df['avg_word_length'] = train_df.apply(lambda row: row['char_count'] / row['word_count'] if row['word_count'] != 0 else 0, axis=1)\n",
        "train_df['avg_sentence_length'] = train_df.apply(lambda row: row['word_count'] / row['sentence_count'] if row['sentence_count'] != 0 else 0, axis=1)\n",
        "\n",
        "\n",
        "# Hyperparameter specifying the limit of words in the vocabulary of the model\n",
        "MAX_NB_WORDS = 29000\n",
        "\n",
        "# Tokenizer with OOV (out of vocabulary)\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, oov_token='OOV')\n",
        "tokenizer.fit_on_texts(train_df['job_description'].values)\n",
        "\n",
        "# Hyperparameter specifying the max number of words in each job description\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Convert job descriptions into sequeneces of digits and apply padding to ensure consistent dimensions\n",
        "X_descr = tokenizer.texts_to_sequences(train_df['job_description'].values)\n",
        "X_descr = pad_sequences(X_descr, maxlen = MAX_SEQUENCE_LENGTH)\n",
        "MAX_NB_WORDS = len(tokenizer.word_index) + 1 # accounting for OOV words."
      ],
      "metadata": {
        "id": "D3xgVZDAZ4eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training for Job Type\n"
      ],
      "metadata": {
        "id": "go2NmLxnZ5cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section, we design and train a model for predicting the job type given a job description."
      ],
      "metadata": {
        "id": "6_Q6wOVaCGH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Job types\n",
        "job_type = {\n",
        "    'Full-time':0,\n",
        "    'Contract':1,\n",
        "    'Part-time':2,\n",
        "    'Temporary':3,\n",
        "    'Internship':4,\n",
        "    'Other':5,\n",
        "    'Volunteer':6,\n",
        "    }\n",
        "\n",
        "# Setting target labels\n",
        "Y_type = pd.get_dummies(train_df.replace({\"job_type\": job_type})['job_type'].values)\n",
        "\n",
        "# Random initialisations of weights\n",
        "class_weights = {i: random.random() for i in range(7)}\n",
        "\n",
        "# Random data splitting for training and evaluating\n",
        "X_train_type, X_test_type, Y_train_type, Y_test_type = train_test_split(X_descr,Y_type, test_size = 0.20)\n",
        "\n",
        "# Hyperparameter specifying the size of the vector for each words in the Dense layer\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "# Creating a sequential model\n",
        "job_model = Sequential()\n",
        "job_model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length = X_train_type.shape[1]))\n",
        "job_model.add(LSTM(100, dropout = 0.3, recurrent_dropout = 0.3, return_sequences=True))\n",
        "job_model.add(LSTM(80, dropout = 0.3, recurrent_dropout = 0.3))\n",
        "job_model.add(Dense(128, activation = 'relu'))\n",
        "job_model.add(Dropout(0.3))\n",
        "job_model.add(Dense(7, activation = 'softmax'))\n",
        "job_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "\n",
        "# Training and evaluating the model\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "history = job_model.fit(\n",
        "    X_train_type,\n",
        "    Y_train_type,\n",
        "    epochs = epochs,\n",
        "    batch_size = batch_size,\n",
        "    validation_split = 0.2,\n",
        "    callbacks = [EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0.0001)],\n",
        "    class_weight = class_weights)\n",
        "\n",
        "print(\"Evaluating model on test set...\")\n",
        "\n",
        "accr = job_model.evaluate(X_test_type, Y_test_type)\n",
        "\n",
        "print('Test set:    Loss: {:0.3f}, Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtxP0LNPeXBJ",
        "outputId": "209ece4c-ef01-4c7a-edb9-66ac4f597d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "333/333 [==============================] - 250s 722ms/step - loss: 0.2633 - accuracy: 0.8051 - val_loss: 0.7780 - val_accuracy: 0.8154\n",
            "Epoch 2/5\n",
            "333/333 [==============================] - 230s 690ms/step - loss: 0.2045 - accuracy: 0.8073 - val_loss: 0.6818 - val_accuracy: 0.8152\n",
            "Epoch 3/5\n",
            "333/333 [==============================] - 233s 701ms/step - loss: 0.1804 - accuracy: 0.8087 - val_loss: 0.6610 - val_accuracy: 0.8201\n",
            "Epoch 4/5\n",
            "333/333 [==============================] - 237s 713ms/step - loss: 0.1635 - accuracy: 0.8192 - val_loss: 0.6564 - val_accuracy: 0.8207\n",
            "Epoch 5/5\n",
            "333/333 [==============================] - 240s 722ms/step - loss: 0.1504 - accuracy: 0.8318 - val_loss: 0.5733 - val_accuracy: 0.8267\n",
            "Evaluating model on test set...\n",
            "208/208 [==============================] - 16s 78ms/step - loss: 0.6074 - accuracy: 0.8186\n",
            "Test set:    Loss: 0.607, Accuracy: 0.819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training for Job Category"
      ],
      "metadata": {
        "id": "5LIS2hNpaf8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section, we design and train a model for predicting the job category given a job description."
      ],
      "metadata": {
        "id": "Wd_3kxzwFDXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Job categories\n",
        "job_cat = {\n",
        "    'IT': 0,\n",
        "    'Marketing': 1,\n",
        "    'Finance': 2,\n",
        "    'Customer Service': 3,\n",
        "    'Healthcare': 4,\n",
        "    'Education': 5,\n",
        "    'Engineering': 6,\n",
        "    'Administration': 7,\n",
        "    'Sales': 8,\n",
        "    'Research': 9,\n",
        "    'Creative': 10,\n",
        "    'Legal': 11,\n",
        "    'Human Resources': 12,\n",
        "    'Manufacturing': 13,\n",
        "    'Consulting': 14,\n",
        "    'Logistics': 15,\n",
        "    'Real Estate': 16,\n",
        "    'uncategorized': 17\n",
        "}\n",
        "\n",
        "# Random weight initialisation\n",
        "class_weights = {i: random.random() for i in range(18)}\n",
        "\n",
        "# Setting target labels\n",
        "Y_cat = pd.get_dummies(train_df.replace({\"category\": job_cat})['category'].values) # target variable\n",
        "\n",
        "# Random data splitting for training and evaluating\n",
        "X_train_cat, X_test_cat, Y_train_cat, Y_test_cat = train_test_split(X_descr, Y_cat, test_size=0.20)\n",
        "\n",
        "# Hyperparameter specifying the size of the vector for each words in the Dense layer\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Creating a sequential model\n",
        "cat_model = Sequential()\n",
        "cat_model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_descr.shape[1]))\n",
        "cat_model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
        "cat_model.add(Dense(128, activation='relu'))\n",
        "cat_model.add(Dropout(0.5))\n",
        "cat_model.add(Dense(18, activation='softmax'))\n",
        "cat_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training and evaluating the model\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "history_cat = cat_model.fit(\n",
        "    X_train_cat,\n",
        "    Y_train_cat,\n",
        "    epochs = epochs,\n",
        "    batch_size = batch_size,\n",
        "    validation_split = 0.2,\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)],\n",
        "    class_weight = class_weights)\n",
        "\n",
        "print('Evaluating model on test set...')\n",
        "\n",
        "accr = cat_model.evaluate(X_test_cat, Y_test_cat)\n",
        "\n",
        "print('Test set:    Loss: {:0.3f}, Accuracy: {:0.3f}'.format(accr[0], accr[1]))"
      ],
      "metadata": {
        "id": "8aKtVn6eajGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6dd4bc-913c-42c1-aa6b-1e639207f00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "333/333 [==============================] - 269s 793ms/step - loss: 0.5567 - accuracy: 0.6555 - val_loss: 0.3678 - val_accuracy: 0.9113\n",
            "Epoch 2/5\n",
            "333/333 [==============================] - 278s 836ms/step - loss: 0.1053 - accuracy: 0.9321 - val_loss: 0.1534 - val_accuracy: 0.9720\n",
            "Epoch 3/5\n",
            "333/333 [==============================] - 262s 788ms/step - loss: 0.0419 - accuracy: 0.9694 - val_loss: 0.1050 - val_accuracy: 0.9780\n",
            "Epoch 4/5\n",
            "333/333 [==============================] - 267s 801ms/step - loss: 0.0231 - accuracy: 0.9795 - val_loss: 0.0910 - val_accuracy: 0.9806\n",
            "Epoch 5/5\n",
            "333/333 [==============================] - 281s 842ms/step - loss: 0.0141 - accuracy: 0.9833 - val_loss: 0.0978 - val_accuracy: 0.9799\n",
            "Evaluating model on test set...\n",
            "208/208 [==============================] - 14s 66ms/step - loss: 0.1015 - accuracy: 0.9771\n",
            "Test set:    Loss: 0.102, Accuracy: 0.977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Letter Text Parser (with ChatGPT)\n"
      ],
      "metadata": {
        "id": "Y2pAVJicMKvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning"
      ],
      "metadata": {
        "id": "kUC3DugjIchP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section we connect to OpenAI's ChatGPT API and fine tune the model."
      ],
      "metadata": {
        "id": "L0M9llOrIx_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ''\n",
        "\n",
        "\n",
        "# Example conversation to fine tune OpenAI's model. In this case, we just specify the content for system, a general attribute describing the context and goal of the model.\n",
        "messages = [\n",
        "\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"\"\"You receive as input a motivational or statement or reference letter written by the user for a job application.\n",
        "        Internally, extract extract these features: Job Type (full time, part time, hybrid), Job Title, Job Position (intership, entry level, associate, midsenior level, director, executive),\n",
        "        Subject of education obtained (excluding univeristy name), Professional experiences, International experiences, Intrapersonal skills, Computer software tools known, Passions, and Distinguishable features.\n",
        "        Then, write a brief job description based on the features you extracted.\"\"\"},\n",
        "    ]"
      ],
      "metadata": {
        "id": "kn4p95pr5-j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ],
      "metadata": {
        "id": "VBmpJhEpKZzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this sub-section, we define two functions that take as input a letter and output the key characteristics."
      ],
      "metadata": {
        "id": "2Zu-CoBpKdJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for extracting the key information from the letter\n",
        "def convert_letter(message):\n",
        "    if message:\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-003\",\n",
        "            prompt=\"\\n\".join([msg['content'] for msg in messages]),\n",
        "            temperature = 0.3, # parameter for randomness of response\n",
        "            max_tokens = 250, # parameter for length of response\n",
        "            stop=None)\n",
        "\n",
        "        reply = response.choices[0].text.strip()\n",
        "\n",
        "        return reply"
      ],
      "metadata": {
        "id": "cu3G8xzd6CBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note for the block below: running the API may return an empty print. If that is the case, run again the block with the same input.  "
      ],
      "metadata": {
        "id": "sc9nXjHmZhhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Usage of OpenAI's ChatGPT API\n",
        "message = input(\"User: \")\n",
        "reply = convert_letter(message)\n",
        "print(reply)\n",
        "\n",
        "job_description = reply"
      ],
      "metadata": {
        "id": "Csbcdt04s0ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting job type and category"
      ],
      "metadata": {
        "id": "A6FWxB-LvJGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we put pass the information extracted from the letter and predict the ideal job type and category of the user."
      ],
      "metadata": {
        "id": "UqkH2ey8JW-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "job_pred = []\n",
        "category_pred = []\n",
        "\n",
        "# Preparing input from motivational letter\n",
        "seq = tokenizer.texts_to_sequences([job_description])\n",
        "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Predicting job type\n",
        "pred_job = job_model.predict(padded)\n",
        "labels_job = ['Full-time', 'Contract', 'Part-time', 'Temporary', 'Internship', 'Other', 'Volunteer']\n",
        "index_job = min(np.argmax(pred_job), len(labels_job) - 1)\n",
        "job_pred.append(labels_job[index_job])\n",
        "\n",
        "# Predicting job category\n",
        "pred_cat = cat_model.predict(padded)\n",
        "labels_cat = ['IT', 'Marketing', 'Finance', 'Customer Service', 'Healthcare', 'Education', 'Engineering', 'Administration',\n",
        "              'Sales', 'Research', 'Creative', 'Legal', 'Human Resources', 'Manufacturing', 'Consulting', 'Logistics', 'Real Estate']\n",
        "index_cat = min(np.argmax(pred_cat), len(labels_cat) - 1)\n",
        "category_pred.append(labels_cat[index_cat])\n",
        "\n",
        "# Saving and visualising results\n",
        "pred_results = pd.DataFrame()\n",
        "pred_results['job_type_pred'] = pd.Series(job_pred)\n",
        "pred_results['job_cat_pred'] = pd.Series(category_pred)\n",
        "pred_results"
      ],
      "metadata": {
        "id": "-k0yfrCkvRck"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}